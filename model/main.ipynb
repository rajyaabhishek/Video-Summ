{"cells":[{"cell_type":"markdown","metadata":{},"source":["#attention"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import math"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import numpy as np\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, input_size=1024, output_size=1024, block_size=60):\n","        \"\"\" The basic Attention 'cell' containing the learnable parameters of Q, K and V.\n","\n","        :param int input_size: Feature input size of Q, K, V.\n","        :param int output_size: Feature -hidden- size of Q, K, V.\n","        :param int block_size: The size of the blocks utilized inside the attention matrix.\n","        \"\"\"\n","        super(SelfAttention, self).__init__()\n","\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.block_size = block_size\n","        self.Wk = nn.Linear(in_features=input_size, out_features=output_size, bias=False)\n","        self.Wq = nn.Linear(in_features=input_size, out_features=output_size, bias=False)\n","        self.Wv = nn.Linear(in_features=input_size, out_features=output_size, bias=False)\n","        self.out = nn.Linear(in_features=output_size+2, out_features=input_size, bias=False)\n","\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    @staticmethod\n","    def get_entropy(logits):\n","        \"\"\" Compute the entropy for each row of the attention matrix.\n","\n","        :param torch.Tensor logits: The raw (non-normalized) attention values with shape [T, T].\n","        :return: A torch.Tensor containing the normalized entropy of each row of the attention matrix, with shape [T].\n","        \"\"\"\n","        _entropy = F.softmax(logits, dim=-1) * F.log_softmax(logits, dim=-1)\n","        _entropy = -1.0 * _entropy.sum(-1)\n","\n","        # https://stats.stackexchange.com/a/207093 Maximum value of entropy is log(k), where k the # of used categories.\n","        # Here k is when all the values of a row is different of each other (i.e., k = # of video frames)\n","        return _entropy / np.log(logits.shape[0])\n","\n","    def forward(self, x):\n","        \"\"\" Compute the weighted frame features, through the Block diagonal sparse attention matrix and the estimates of\n","        the frames attentive uniqueness and the diversity.\n","\n","        :param torch.Tensor x: Frame features with shape [T, input_size].\n","        :return: A tuple of:\n","                    y: The computed weighted features, with shape [T, input_size].\n","                    att_win : The Block diagonal sparse attention matrix, with shape [T, T].\n","        \"\"\"\n","        # Compute the pairwise dissimilarity of each frame, on the initial feature space (GoogleNet features)\n","        x_unit = F.normalize(x, p=2, dim=1)\n","        similarity = x_unit @ x_unit.t()\n","        diversity = 1 - similarity\n","\n","        K = self.Wk(x)\n","        Q = self.Wq(x)\n","        V = self.Wv(x)\n","\n","        energies = torch.matmul(Q, K.transpose(1, 0))\n","        att_weights = self.softmax(energies)\n","\n","        # Entropy is a measure of uncertainty: Higher value means less information.\n","        entropy = self.get_entropy(logits=energies)\n","        entropy = F.normalize(entropy, p=1, dim=-1)\n","\n","        # Compute the mask to form the Block diagonal sparse attention matrix\n","        D = self.block_size\n","        num_blocks = math.ceil(energies.shape[0] / D)\n","        keepingMask = torch.ones(num_blocks, D, D, device=att_weights.device)\n","        keepingMask = torch.block_diag(*keepingMask)[:att_weights.shape[0], :att_weights.shape[0]]\n","        zeroingMask = (1 - keepingMask)\n","        att_win = att_weights * keepingMask\n","\n","        # Pick those frames that are \"invisible\" to a frame, aka outside the block (mask)\n","        attn_remainder = att_weights * zeroingMask\n","        div_remainder = diversity * zeroingMask\n","\n","        # Compute non-local dependencies based on the diversity of those frames\n","        dep_factor = (div_remainder * attn_remainder).sum(-1).div(div_remainder.sum(-1))\n","        dep_factor = dep_factor.unsqueeze(0).expand(dep_factor.shape[0], -1)\n","        masked_dep_factor = dep_factor * keepingMask\n","        att_win += masked_dep_factor\n","\n","        y = torch.matmul(att_win, V)\n","        characteristics = (entropy, dep_factor[0, :])\n","        characteristics = torch.stack(characteristics).detach()\n","        outputs = torch.cat(tensors=(y, characteristics.t()), dim=-1)\n","\n","        y = self.out(outputs)\n","        return y, att_win.clone()\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Output shape: torch.Size([500, 256])\tattention shape: torch.Size([500, 500])\n"]}],"source":["if __name__ == '__main__':\n","    pass\n","    # Uncomment for a quick proof of concept\n","    model = SelfAttention(input_size=256, output_size=128, block_size=30)\n","    _input = torch.randn(500, 256)  # [seq_len, hidden_size]\n","    output, weights = model(_input)\n","    print(f\"Output shape: {output.shape}\\tattention shape: {weights.shape}\")\n","            "]},{"cell_type":"markdown","metadata":{},"source":["#summarizer"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class CA_SUM(nn.Module):\n","    def __init__(self, input_size=1024, output_size=1024, block_size=60):\n","        \"\"\" Class wrapping the CA-SUM model; its key modules and parameters.\n","        \n","        :param int input_size: The expected input feature size.\n","        :param int output_size: The produced output feature size.\n","        :param int block_size: The size of the blocks utilized inside the attention matrix.\n","        \"\"\"\n","        super(CA_SUM, self).__init__()\n","        self.attention = SelfAttention(input_size=input_size, output_size=output_size, block_size=block_size)\n","        self.linear_1 = nn.Linear(in_features=input_size, out_features=input_size)\n","        self.linear_2 = nn.Linear(in_features=self.linear_1.out_features, out_features=1)\n","        self.drop = nn.Dropout(p=0.5)\n","        self.norm_y = nn.LayerNorm(normalized_shape=input_size, eps=1e-6)\n","        self.norm_linear = nn.LayerNorm(normalized_shape=self.linear_1.out_features, eps=1e-6)\n","        self.relu = nn.ReLU()\n","        self.sigmoid = nn.Sigmoid()\n","    def forward(self, frame_features):\n","        \"\"\" Produce frame-level importance scores from the frame features, using the CA-SUM model.\n","        :param torch.Tensor frame_features: Tensor of shape [T, input_size] containing the frame features produced by \n","        using the pool5 layer of GoogleNet.\n","        :return: A tuple of:\n","            y: Tensor with shape [1, T] containing the frames importance scores in [0, 1].\n","            attn_weights: Tensor with shape [T, T] containing the attention weights.\n","        \"\"\"\n","        residual = frame_features\n","        weighted_value, attn_weights = self.attention(frame_features)\n","        y = residual + weighted_value\n","        y = self.drop(y)\n","        y = self.norm_y(y)\n","\n","        # 2-layer NN (Regressor Network)\n","        y = self.linear_1(y)\n","        y = self.relu(y)\n","        y = self.drop(y)\n","        y = self.norm_linear(y)\n","        y = self.linear_2(y)\n","        y = self.sigmoid(y)\n","        y = y.view(1, -1)\n","        return y, attn_weights\n","    "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Output shape: torch.Size([1, 500])\tattention shape: torch.Size([500, 500])\n"]}],"source":["if __name__ == '__main__':\n","    pass\n","    # Uncomment for a quick proof of concept\n","    model = CA_SUM(input_size=256, output_size=128, block_size=30)\n","    _input = torch.randn(500, 256)  # [seq_len, hidden_size]\n","    output, weights = model(_input)\n","    print(f\"Output shape: {output.shape}\\tattention shape: {weights.shape}\")\n","    "]},{"cell_type":"markdown","metadata":{},"source":["#evaluation_metrics"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","import numpy as np\n","import csv\n","import sys\n","import os\n","from os import listdir\n","from scipy.stats import spearmanr, kendalltau, rankdata\n","from collections import Counter"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[],"source":["def evaluate_summary(predicted_summary, user_summary, eval_method):\n","    \"\"\" Compare the predicted summary with the user defined one(s).\n","    :param np.ndarray predicted_summary: The generated summary from our model.\n","    :param np.ndarray user_summary: The user defined ground truth summaries (or summary).\n","    :param str eval_method: The proposed evaluation method; either 'max' (SumMe) or 'avg' (TVSum).\n","    :return: The reduced fscore based on the eval_method\n","    \"\"\"\n","    max_len = max(len(predicted_summary), user_summary.shape[1])\n","    S = np.zeros(max_len, dtype=int)\n","    G = np.zeros(max_len, dtype=int)\n","    S[:len(predicted_summary)] = predicted_summary\n","    f_scores = []\n","    for user in range(user_summary.shape[0]):\n","        G[:user_summary.shape[1]] = user_summary[user]\n","        overlapped = S & G\n","        \n","        # Compute precision, recall, f-score\n","        precision = sum(overlapped)/sum(S)\n","        recall = sum(overlapped)/sum(G)\n","        if precision+recall == 0:\n","            f_scores.append(0)\n","        else:\n","            f_scores.append(2 * precision * recall * 100 / (precision + recall))\n","    if eval_method == 'max':\n","        return max(f_scores)\n","    else:\n","        return sum(f_scores)/len(f_scores)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def get_corr_coeff(pred_imp_scores, video, dataset):\n","    \"\"\" Read users annotations (frame-level importance scores) for the `video` of the `dataset`* in use. Compare the\n","    multiple user annotations for the test video with the predicted frame-level importance scores of our CA-SUM for the\n","    same video, by computing the Spearman's rho and Kendall's tau correlation coefficients. It must be noted, that the\n","    calculated values are the average correlation coefficients over the multiple annotators.\n","    * Applicable only for the TVSum dataset.\n","    :param list[float] pred_imp_scores: The predicted frame-level importance scores from our CA-SUM model.\n","    :param str video: The name of the test video being inferenced.\n","    :param str dataset: The dataset in use.\n","    :return: A tuple containing the video-level Spearman's rho and Kendall's tau correlation coefficients.\n","    \"\"\"\n","\n","    # Read the user annotations from the file\n","    annot_path = f\"C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/data/{dataset}/ydata-anno.tsv\"\n","    with open(annot_path) as annot_file:\n","        user = int(video.split(\"_\")[-1])\n","        annot = list(csv.reader(annot_file, delimiter=\"\\t\"))\n","        annotation_length = list(Counter(np.array(annot)[:, 0]).values())[user-1]\n","        init = (user - 1) * annotation_length\n","        till = user * annotation_length\n","        user_scores = []\n","        for row in annot[init:till]:\n","            curr_user_score = row[2].split(\",\")\n","            curr_user_score = np.array([float(num) for num in curr_user_score])\n","            curr_user_score = curr_user_score / curr_user_score.max(initial=-1)  # Normalize scores between 0 and 1\n","            curr_user_score = curr_user_score[::15]\n","            user_scores.append(curr_user_score)\n","    pred_imp_scores = np.array(pred_imp_scores)\n","    rho_coeff, tau_coeff = [], []\n","    for annot in range(len(user_scores)):\n","        true_user_score = user_scores[annot]\n","        curr_rho_coeff, _ = spearmanr(pred_imp_scores, true_user_score)\n","        curr_tau_coeff, _ = kendalltau(rankdata(pred_imp_scores), rankdata(true_user_score))\n","        rho_coeff.append(curr_rho_coeff)\n","        tau_coeff.append(curr_tau_coeff)\n","    rho_coeff = np.array(rho_coeff).mean()  # mean over all user annotations\n","    tau_coeff = np.array(tau_coeff).mean()  # mean over all user annotations\n","    return rho_coeff, tau_coeff"]},{"cell_type":"markdown","metadata":{},"source":["#compute_fscores"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["\n","args={\"path\":r\"C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\exp1\\reg0.6\\SumMe\\results\\split0\",\n","       \"dataset\":\"SumMe\",\n","       \"eval\":\"max\"\n","}\n","\n","path = args[\"path\"]\n","dataset = args[\"dataset\"]\n","eval_method = args[\"eval\"]\n","\n","results = [f for f in path if f.endswith(\".json\")]\n","results.sort(key=lambda video: int(video[6:-5]))\n","dataset_path = 'C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/data/' + dataset + '/eccv16_dataset_' + dataset.lower() + '_google_pool5.h5'"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["f_score_epochs = []\n","for epoch in results:                       # for each epoch ...\n","    all_scores = []\n","    with open(path + '/' + epoch) as f:     # read the json file ...\n","        data = json.loads(f.read())\n","        keys = list(data.keys())\n","        for video_name in keys:                    # for each video inside that json file ...\n","            scores = np.asarray(data[video_name])  # read the importance scores from frames\n","            all_scores.append(scores)\n","    all_user_summary, all_shot_bound, all_nframes, all_positions = [], [], [], []\n","    with h5py.File(dataset_path, 'r') as hdf:\n","        for video_name in keys:\n","            user_summary = np.array(hdf.get(video_name + '/user_summary'))\n","            sb = np.array(hdf.get(video_name + '/change_points'))\n","            n_frames = np.array(hdf.get(video_name + '/n_frames'))\n","            positions = np.array(hdf.get(video_name + '/picks'))\n","            all_user_summary.append(user_summary)\n","            all_shot_bound.append(sb)\n","            all_nframes.append(n_frames)\n","            all_positions.append(positions)\n","    all_summaries = generate_summary(all_shot_bound, all_scores, all_nframes, all_positions)\n","    all_f_scores = []\n","    # compare the resulting summary with the ground truth one, for each video\n","    for video_index in range(len(all_summaries)):\n","        summary = all_summaries[video_index]\n","        user_summary = all_user_summary[video_index]\n","        f_score = evaluate_summary(summary, user_summary, eval_method)\n","        all_f_scores.append(f_score)\n","    f_score_epochs.append(np.mean(all_f_scores))\n","    num_epoch = epoch.split(\".\")[0][6:]\n","    print(f\"[epoch {num_epoch}] f_score: {np.mean(all_f_scores)}\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["with open(path + '/f_scores.txt', 'w') as outfile:\n","    for f_score in f_score_epochs:\n","        outfile.write('%s\\n' % f_score)"]},{"cell_type":"markdown","metadata":{},"source":["#choose_best_model"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","import numpy as np\n","import csv\n","import json\n","import sys\n","from scipy.stats import spearmanr, kendalltau, rankdata"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[],"source":["exp_num =sys.argv[1]   #changes\n","# exp_num =1\n","dataset = args[\"dataset\"]"]},{"cell_type":"code","execution_count":118,"metadata":{},"outputs":[],"source":["base_path = \"C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/summaries\"\n","eligible_datasets = [\"TVSum\"]"]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[],"source":["def get_corr_coeff(epoch, split_id, reg_factor):\n","    \"\"\" Read users annotations (frame-level importance scores) for each video in the dataset*. Compare the multiple\n","    user annotations for each test video with the predicted frame-level importance scores of our CA-SUM for the same\n","    video, by computing the Spearman's rho and Kendall's tau correlation coefficients. It must be noted, that for each\n","    test video the calculated values are the average correlation coefficients over the multiple annotators. The final\n","    split-level values are the average over the entire test set.\n","    * Applicable only for the TVSum dataset.\n","    :param int epoch: The chosen training epoch for the given split and regularization factor.\n","    :param int split_id: The id of the current evaluated split.\n","    :param float reg_factor: The value of the current evaluated length regularization factor.\n","    :return: A tuple containing the split-level Spearman's rho and Kendall's tau correlation coefficients.\n","    \"\"\"\n","    if dataset not in eligible_datasets:\n","        print(f\"Correlation coefficients are not supported by {dataset} dataset.\")\n","        return None, None\n","\n","    # Read the user annotations from the file\n","    annot_path = f\"C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/data/{dataset}/ydata-anno.tsv\"\n","    with open(annot_path) as annot_file:\n","        annot = csv.reader(annot_file, delimiter=\"\\t\")\n","        names, user_scores = [], {}\n","        for row in annot:\n","            str_user = row[0]\n","            curr_user_score = row[2].split(\",\")\n","            curr_user_score = np.array([float(num) for num in curr_user_score])\n","            curr_user_score = curr_user_score / curr_user_score.max(initial=-1)  # Normalize scores between 0 and 1\n","            curr_user_score = curr_user_score[::15]\n","            if str_user not in names:\n","                names.append(str_user)\n","                user = f\"video_{len(names)}\"\n","                user_scores[user] = [curr_user_score]\n","            else:\n","                user_scores[user].append(curr_user_score)\n","\n","    # Read each score and compared it\n","    scores_path = f\"{base_path}/exp{exp_num}/reg{reg_factor}/{dataset}/results/split{split_id}/{dataset}_{epoch-1}.json\"\n","    with open(scores_path) as score_file:       # Read the importance scores affiliated with the selected epoch\n","        scores = json.loads(score_file.read())\n","        keys = list(scores.keys())\n","    rho_coeff_video, tau_coeff_video = [], []\n","    for video in keys:\n","        pred_imp_score = np.array(scores[video])\n","        curr_user_scores = user_scores[video]\n","        rho_coeff, tau_coeff = [], []\n","        for annot in range(len(curr_user_scores)):\n","            true_user_score = curr_user_scores[annot]\n","            curr_rho_coeff, _ = spearmanr(pred_imp_score, true_user_score)\n","            curr_tau_coeff, _ = kendalltau(rankdata(pred_imp_score), rankdata(true_user_score))\n","            rho_coeff.append(curr_rho_coeff)\n","            tau_coeff.append(curr_tau_coeff)\n","        rho_coeff = np.array(rho_coeff).mean()  # mean over all user annotations\n","        rho_coeff_video.append(rho_coeff)\n","        tau_coeff = np.array(tau_coeff).mean()  # mean over all user annotations\n","        tau_coeff_video.append(tau_coeff)\n","    rho_coeff_split = np.array(rho_coeff_video).mean()  # mean over all videos\n","    tau_coeff_split = np.array(tau_coeff_video).mean()  # mean over all videos\n","    return rho_coeff_split, tau_coeff_split"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[],"source":["def get_improvement_score(epoch, split_id, reg_factor):\n","    \"\"\" Using the estimated frame-level importance scores from an untrained model, calculate the improvement (eq. 2-3)\n","    of a  trained model for the chosen epoch, on a given split and regularization factor.\n","    :param int epoch: The chosen training epoch for the given split and regularization factor.\n","    :param int split_id: The id of the current evaluated split.\n","    :param float reg_factor: The value of the current evaluated length regularization factor\n","    :return: The relative improvement of a trained model over an untrained (random) one.\n","    \"\"\"\n","    untr_path = f\"{base_path}/exp{exp_num}/reg{reg_factor}/{dataset}/results/split{split_id}/{dataset}_-1.json\"\n","    curr_path = f\"{base_path}/exp{exp_num}/reg{reg_factor}/{dataset}/results/split{split_id}/{dataset}_{epoch}.json\"\n","    with open(curr_path) as curr_file, open(untr_path) as untr_file:\n","        untr_data = json.loads(untr_file.read())\n","        curr_data = json.loads(curr_file.read())\n","        keys = list(curr_data.keys())\n","        mean_untr_scores, mean_curr_scores = [], []\n","        for video_name in keys:                              # For a video inside that split get the ...\n","            untr_scores = np.asarray(untr_data[video_name])  # Untrained model computed importance scores\n","            curr_scores = np.asarray(curr_data[video_name])  # trained model computed importance scores\n","            mean_untr_scores.append(np.mean(untr_scores))\n","            mean_curr_scores.append(np.mean(curr_scores))\n","    mean_untr_scores = np.array(mean_untr_scores)\n","    mean_curr_scores = np.array(mean_curr_scores)\n","\n","    # Measure how much did we improve a random model, relatively to moving towards sigma (minimum loss)\n","    improvement = abs(mean_curr_scores.mean() - mean_untr_scores.mean())\n","    result = (improvement / abs(reg_factor - mean_untr_scores.mean()))\n","    return result"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[],"source":["def train_logs(log_file, method=\"argmin\"):\n","    \"\"\" Choose and return the epoch based only on the training loss. Through the `method` argument you can get the epoch\n","    associated with the minimum training loss (argmin) or the last epoch of the training process (last).\n","    :param str log_file: Path to the saved csv file containing the loss information.\n","    :param str method: The chosen criterion for the epoch (model) picking process.\n","    :return: The epoch of the best model, according to the chosen criterion.\n","    \"\"\"\n","    losses = {}\n","    losses_names = []\n","\n","    # Read the csv file with the training losses\n","    with open(log_file) as csv_file:\n","        csv_reader = csv.reader(csv_file, delimiter=',')\n","        for (i, row) in enumerate(csv_reader):\n","            if i == 0:\n","                for col in range(len(row)):\n","                    losses[row[col]] = []\n","                    losses_names.append(row[col])\n","            else:\n","                for col in range(len(row)):\n","                    losses[losses_names[col]].append(float(row[col]))\n","\n","    # criterion: The length regularization of the generated summary (400 epochs, after which overfitting problems occur)\n","    loss = losses[\"loss_epoch\"]\n","    loss = loss[:400]          \n","    START_EPOCH = 20                      # If unstable training is observed at the start\n","    if method == \"last\":\n","        epoch = len(loss) - 1\n","    elif method == \"argmin\":\n","        epoch = np.array(loss[START_EPOCH:]).argmin() + START_EPOCH\n","    else:\n","        raise ValueError(f\"Method {method} is not currently supported. Only `last` and `argmin` are available.\")\n","    return epoch"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[{"ename":"OSError","evalue":"[Errno 22] Invalid argument: 'C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/summaries/exp1/reg0.5/C:\\\\Users\\\\abhis\\\\OneDrive\\\\Desktop\\\\video summarization\\\\CA-SUM-main\\\\data\\\\SumMe\\\\eccv16_dataset_summe_google_pool5.h5/logs/split0/scalers.csv'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[1;32mIn[123], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      7\u001b[0m     log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/exp\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/reg\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msigma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/logs/split\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/scalers.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     selected_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# w/o +1. (only needed to pick the f-score value)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     split_improvements[split] \u001b[38;5;241m=\u001b[39m get_improvement_score(epoch\u001b[38;5;241m=\u001b[39mselected_epoch, split_id\u001b[38;5;241m=\u001b[39msplit, reg_factor\u001b[38;5;241m=\u001b[39msigma)\n\u001b[0;32m     10\u001b[0m     split_epochs[split] \u001b[38;5;241m=\u001b[39m selected_epoch\n","Cell \u001b[1;32mIn[122], line 12\u001b[0m, in \u001b[0;36mtrain_logs\u001b[1;34m(log_file, method)\u001b[0m\n\u001b[0;32m      9\u001b[0m losses_names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Read the csv file with the training losses\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csv_file:\n\u001b[0;32m     13\u001b[0m     csv_reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(csv_file, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (i, row) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(csv_reader):\n","File \u001b[1;32mc:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/summaries/exp1/reg0.5/C:\\\\Users\\\\abhis\\\\OneDrive\\\\Desktop\\\\video summarization\\\\CA-SUM-main\\\\data\\\\SumMe\\\\eccv16_dataset_summe_google_pool5.h5/logs/split0/scalers.csv'"]}],"source":["# Choose the model associated with the min training loss for each regularization factor and get its improvement score\n","all_improvements, all_epochs = [], []\n","sigmas = [i/10 for i in range(5, 10)]  # The valid values for the length regularization factor\n","for sigma in sigmas:\n","    split_improvements, split_epochs = np.zeros(5, dtype=float), np.zeros(5, dtype=int)\n","    for split in range(0, 5):\n","        log = f\"{base_path}/exp{exp_num}/reg{sigma}/{dataset}/logs/split{split}/scalers.csv\"\n","        selected_epoch = train_logs(log, method=\"argmin\")  # w/o +1. (only needed to pick the f-score value)\n","        split_improvements[split] = get_improvement_score(epoch=selected_epoch, split_id=split, reg_factor=sigma)\n","        split_epochs[split] = selected_epoch\n","    all_improvements.append(split_improvements)\n","    all_epochs.append(split_epochs)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"need at least one array to stack","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[110], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_improvements \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_improvements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m all_epochs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(all_epochs)\n","File \u001b[1;32mc:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\shape_base.py:445\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[0;32m    443\u001b[0m arrays \u001b[38;5;241m=\u001b[39m [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneed at least one array to stack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    447\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n","\u001b[1;31mValueError\u001b[0m: need at least one array to stack"]}],"source":["all_improvements = np.stack(all_improvements)\n","all_epochs = np.stack(all_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"'>' not supported between instances of 'list' and 'float'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_improvements \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[43mall_improvements\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.5\u001b[39;49m, \u001b[38;5;241m0\u001b[39m, all_improvements)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_improvements)\n\u001b[0;32m      3\u001b[0m improvement_per_spit \u001b[38;5;241m=\u001b[39m all_improvements\u001b[38;5;241m.\u001b[39mmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'list' and 'float'"]}],"source":["all_improvements = np.where(all_improvements > 1.5, 0, all_improvements)\n","print(all_improvements)\n","improvement_per_spit = all_improvements.max(axis=0, initial=-1)\n","chosen_indices = all_improvements.argmax(axis=0)\n","sigma_per_split = np.array(sigmas)[chosen_indices]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'sigma_per_split' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[133], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m all_fscores, all_rho_coeff, all_tau_coeff \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m), np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m), np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     curr_sigma \u001b[38;5;241m=\u001b[39m \u001b[43msigma_per_split\u001b[49m[split]\n\u001b[0;32m      4\u001b[0m     curr_epoch \u001b[38;5;241m=\u001b[39m all_epochs[chosen_indices[split], split] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# because of the evaluation on the untrained model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Read the fscore values\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'sigma_per_split' is not defined"]}],"source":["all_fscores, all_rho_coeff, all_tau_coeff = np.zeros(5, dtype=float), np.zeros(5, dtype=float), np.zeros(5, dtype=float)\n","for split in range(0, 5):\n","    curr_sigma = sigma_per_split[split]\n","    curr_epoch = all_epochs[chosen_indices[split], split] + 1  # because of the evaluation on the untrained model\n","\n","    # Read the fscore values\n","    results_file = f\"{base_path}/exp{exp_num}/reg{curr_sigma}/{dataset}/results/split{split}/f_scores.txt\"\n","    with open(results_file) as f:\n","        f_scores = f.read().strip()  # read F-Scores\n","        if \"\\n\" in f_scores:\n","            f_scores = f_scores.splitlines()\n","        else:\n","            f_scores = json.loads(f_scores)\n","    f_scores = np.array([float(f_score) for f_score in f_scores])\n","    curr_fscore = np.round(f_scores[curr_epoch], 2)\n","    all_fscores[split] = curr_fscore\n","    print(f\"[Split: {split}] Fscore: {curr_fscore:.2f}\", end=\"\")\n","\n","    # Compute correlation coefficients\n","    if dataset in eligible_datasets:    \n","        rho, tau = get_corr_coeff(epoch=curr_epoch, split_id=split, reg_factor=curr_sigma)\n","        all_rho_coeff[split] = rho\n","        all_tau_coeff[split] = tau\n","        print(f\"  Spearman's \\u03C1: {rho:.3f}  Kendall's \\u03C4: {tau:.3f}\", end=\"\")\n","    print(f\" [\\u03C3={curr_sigma}, epoch: {curr_epoch}]\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'all_fscores' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[82], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m avg_fscore \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mall_fscores\u001b[49m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m eligible_datasets:\n\u001b[0;32m      3\u001b[0m     avg_rho, avg_tau \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mround(np\u001b[38;5;241m.\u001b[39mmean(all_rho_coeff), \u001b[38;5;241m2\u001b[39m), np\u001b[38;5;241m.\u001b[39mround(np\u001b[38;5;241m.\u001b[39mmean(all_tau_coeff), \u001b[38;5;241m2\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'all_fscores' is not defined"]}],"source":["avg_fscore = np.round(np.mean(all_fscores), 2)\n","if dataset in eligible_datasets:\n","    avg_rho, avg_tau = np.round(np.mean(all_rho_coeff), 2), np.round(np.mean(all_tau_coeff), 2)\n","    print(\"====================================================================================\")\n","    print(f\"Avg values :=> F1: {avg_fscore}  Spearman's \\u03C1: {avg_rho:.3f}  Kendall's \\u03C4: {avg_tau:.3f}\")\n","else:\n","    print(f\"Avg values :=> F1: {avg_fscore:.2f}\")"]},{"cell_type":"markdown","metadata":{},"source":["#inference"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["from os import listdir\n","from os.path import isfile, join\n","import h5py\n","import json\n","import argparse"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["eligible_datasets = [\"TVSum\"]"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def str2bool(v):\n","    \"\"\" Transcode string to boolean.\n","    :param str v: String to be transcoded.\n","    :return: The boolean transcoding of the string.\n","    \"\"\"\n","    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n","        return True\n","    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n","        return False\n","    else:\n","        raise argparse.ArgumentTypeError('Boolean value expected.')"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["def inference(model, data_path, keys, eval_method):\n","    \"\"\" Used to inference a pretrained `model` on the `keys` test videos, based on the `eval_method` criterion; using\n","        the dataset located in `data_path'.\n","        :param nn.Module model: Pretrained model to be inferenced.\n","        :param str data_path: File path for the dataset in use.\n","        :param list keys: Containing the test video keys of the used data split.\n","        :param str eval_method: The evaluation method in use {SumMe: max, TVSum: avg}.\n","    \"\"\"\n","    model.eval()\n","    video_fscores, video_rho, video_tau = [], [], []\n","    for video in keys:\n","        with h5py.File(data_path, \"r\") as hdf:\n","            # Input features for inference\n","            frame_features = torch.Tensor(np.array(hdf[f\"{video}/features\"])).view(-1, 1024)\n","            frame_features = frame_features.to(model.linear_1.weight.device)\n","\n","            # Input need for evaluation\n","            user_summary = np.array(hdf[f\"{video}/user_summary\"])\n","            sb = np.array(hdf[f\"{video}/change_points\"])\n","            n_frames = np.array(hdf[f\"{video}/n_frames\"])\n","            positions = np.array(hdf[f\"{video}/picks\"])\n","        with torch.no_grad():\n","            scores, _ = model(frame_features)  # [1, seq_len]\n","            scores = scores.squeeze(0).cpu().numpy().tolist()\n","            summary = generate_summary([sb], [scores], [n_frames], [positions])[0]\n","            f_score = evaluate_summary(summary, user_summary, eval_method)\n","            video_fscores.append(f_score)\n","            if dataset in eligible_datasets and corr_coef:\n","                rho, tau = get_corr_coeff(pred_imp_scores=scores, video=video, dataset=dataset)\n","                video_rho.append(rho)\n","                video_tau.append(tau)\n","    print(f\"CA-SUM model trained for split: {split_id} achieved an F-score: {np.mean(video_fscores):.2f}%\", end=\"\")\n","    if dataset not in eligible_datasets or not corr_coef:\n","        print(\"\\n\", end=\"\")\n","    else:\n","        print(f\", a Spearman's \\u03C1: {np.mean(video_rho):.3f}  and a Kendall's \\u03C4: {np.mean(video_tau):.3f}\")"]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [--dataset DATASET] [--corr_coef CORR_COEF]\n","ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\abhis\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-82521E8ygEXgZCeV.json\n"]},{"ename":"SystemExit","evalue":"2","output_type":"error","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"]}],"source":["if __name__ == \"__main__\":\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    # arguments to run the script\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--dataset\", type=str, default='SumMe', help=\"Dataset to be used. Supported: {SumMe, TVSum}\")\n","    parser.add_argument(\"--corr_coef\", type=str2bool, default=False, help=\"Calculate or not, the correlation coefficients\")\n","    args = vars(parser.parse_args())\n","    dataset = args[\"dataset\"]\n","    corr_coef = args[\"corr_coef\"]\n","    eval_metric = 'avg' if dataset.lower() == 'tvsum' else 'max'\n","    for split_id in range(5):\n","        # # Model data\n","        model_path = f\"C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/inference/pretrained_models/{dataset}/split{split_id}\"\n","        model_file = [f for f in listdir(model_path) if isfile(join(model_path, f))]\n","\n","        # Read current split\n","        split_file = f\"C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/data/splits/{dataset.lower()}_splits.json\"\n","        with open(split_file) as f:\n","            data = json.loads(f.read())\n","            test_keys = data[split_id][\"test_keys\"]\n","\n","        # Dataset path\n","        dataset_path = f\"C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/data/{dataset}/eccv16_dataset_{dataset.lower()}_google_pool5.h5\"\n","\n","        # Create model with paper reported configuration\n","        trained_model = CA_SUM(input_size=1024, output_size=1024, block_size=60).to(device)\n","        trained_model.load_state_dict(torch.load(join(model_path, model_file[-1])))\n","        inference(trained_model, dataset_path, test_keys, eval_metric)\n","        "]},{"cell_type":"markdown","metadata":{},"source":["#exportTensorFlowLog"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","# author: Anders Krogh Mortensen (GitHub: @anderskm)\n","# link: https://github.com/anderskm/exportTensorFlowLog/blob/master/exportTensorFlowLog.py\n","import time\n","import csv\n","import sys\n","import os\n","try:\n","    import collections.abc as collections\n","except ImportError:\n","    import collections"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# Import the event accumulator from Tensorboard. Location varies between Tensorflow versions.<br>\n","# Try each known location until one works.\n","eventAccumulatorImported = False"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["# TF version < 1.1.0\n","import tensorflow.python\n","if not eventAccumulatorImported:\n","    try:\n","        from tensorflow.python.summary import event_accumulator\n","        eventAccumulatorImported = True\n","    except ImportError:\n","        eventAccumulatorImported = False"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# TF version = 1.1.0\n","if not eventAccumulatorImported:\n","    try:\n","        from tensorboard.backend.event_processing import event_accumulator\n","        eventAccumulatorImported = True\n","    except ImportError:\n","        eventAccumulatorImported = False"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["\n","# TF version >= 1.3.0\n","if not eventAccumulatorImported:\n","    try:\n","        from tensorboard.backend.event_processing import event_accumulator\n","        eventAccumulatorImported = True\n","    except ImportError:\n","        eventAccumulatorImported = False"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# TF version = Unknown\n","if not eventAccumulatorImported:\n","    raise ImportError('Could not locate and import Tensorflow event accumulator.')"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["summariesDefault = ['scalars']  # ['scalars', 'histograms', 'images', 'audio', 'compressedHistograms']"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["class Timer(object):\n","    # link: https://stackoverflow.com/a/5849861\n","    def __init__(self, name=None):\n","        self.name = name\n","    def __enter__(self):\n","        self.tStart = time.time()\n","    def __exit__(self, exc_type, value, exc_traceback):\n","        if self.name:\n","            print('[%s]' % self.name)\n","            print('Elapsed: %s' % (time.time() - self.tStart))"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["def exitWithUsage():\n","    print(' ')\n","    print('Usage:')\n","    print('   python readLogs.py <output-folder> <output-path-to-csv> <summaries>')\n","    print('Inputs:')\n","    print('   <input-path-to-logfile>  - Path to TensorFlow logfile.')\n","    print('   <output-folder>          - Path to output folder.')\n","    print(\n","        '   <summaries>              - (Optional) Comma separated list of summaries to save in output-folder. Default: '\n","        + ', '.join(summariesDefault))\n","    print(' ')\n","    sys.exit()"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" \n","Usage:\n","   python readLogs.py <output-folder> <output-path-to-csv> <summaries>\n","Inputs:\n","   <input-path-to-logfile>  - Path to TensorFlow logfile.\n","   <output-folder>          - Path to output folder.\n","   <summaries>              - (Optional) Comma separated list of summaries to save in output-folder. Default: scalars\n"," \n"]},{"ename":"SystemExit","evalue":"","output_type":"error","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[1;31mSystemExit\u001b[0m\n"]}],"source":["if len(sys.argv) < 3:\n","    exitWithUsage()             "]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[{"ename":"IndexError","evalue":"list index out of range","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[106], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m inputLogFile \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m outputFolder \u001b[38;5;241m=\u001b[39m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n","\u001b[1;31mIndexError\u001b[0m: list index out of range"]}],"source":["inputLogFile = sys.argv[1]\n","outputFolder = sys.argv[2]"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["if len(sys.argv) < 4:\n","    summaries = summariesDefault\n","else:\n","    if sys.argv[3] == 'all':\n","        summaries = summariesDefault\n","    else:\n","        summaries = sys.argv[3].split(',')"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["if any(x not in summariesDefault for x in summaries):\n","    print('Unknown summary! See usage for acceptable summaries.')\n","    exitWithUsage()"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"module 'tensorflow' has no attribute 'compat'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[1;32mIn[116], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Setting up event accumulator...\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Timer():\n\u001b[1;32m----> 3\u001b[0m     ea \u001b[38;5;241m=\u001b[39m \u001b[43mevent_accumulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEventAccumulator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputLogFile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43msize_guidance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mevent_accumulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOMPRESSED_HISTOGRAMS\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 0 = grab all\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mevent_accumulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMAGES\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mevent_accumulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUDIO\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mevent_accumulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCALARS\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mevent_accumulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHISTOGRAMS\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorboard\\backend\\event_processing\\event_accumulator.py:312\u001b[0m, in \u001b[0;36mEventAccumulator.__init__\u001b[1;34m(self, path, size_guidance, compression_bps, purge_orphaned_data)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator_mutex \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m--> 312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator \u001b[38;5;241m=\u001b[39m \u001b[43m_GeneratorFromPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression_bps \u001b[38;5;241m=\u001b[39m compression_bps\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpurge_orphaned_data \u001b[38;5;241m=\u001b[39m purge_orphaned_data\n","File \u001b[1;32mc:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorboard\\backend\\event_processing\\event_accumulator.py:944\u001b[0m, in \u001b[0;36m_GeneratorFromPath\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path:\n\u001b[0;32m    943\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath must be a valid string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mio_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIsSummaryEventsFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m event_file_loader\u001b[38;5;241m.\u001b[39mLegacyEventFileLoader(path)\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorboard\\backend\\event_processing\\io_wrapper.py:69\u001b[0m, in \u001b[0;36mIsSummaryEventsFile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mIsSummaryEventsFile\u001b[39m(path):\n\u001b[0;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check whether the path is probably a TF Events file containing Summary.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m        https://github.com/tensorflow/tensorboard/issues/2084.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIsTensorFlowEventsFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.profile-empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorboard\\backend\\event_processing\\io_wrapper.py:53\u001b[0m, in \u001b[0;36mIsTensorFlowEventsFile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath must be a nonempty string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtfevents\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241m.\u001b[39mas_str_any(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(path))\n","File \u001b[1;32mc:\\Users\\abhis\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorboard\\lazy.py:65\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.LazyModule.__getattr__\u001b[1;34m(self, attr_name)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mload_once\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_name\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'compat'"]}],"source":["# Setting up event accumulator...\n","with Timer():\n","    ea = event_accumulator.EventAccumulator(inputLogFile,\n","                                            size_guidance={\n","                                                event_accumulator.COMPRESSED_HISTOGRAMS: 0,  # 0 = grab all\n","                                                event_accumulator.IMAGES: 0,\n","                                                event_accumulator.AUDIO: 0,\n","                                                event_accumulator.SCALARS: 0,\n","                                                event_accumulator.HISTOGRAMS: 0, \n","                                            })"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'ea' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[40], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Loading events from file...\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Timer():\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mea\u001b[49m\u001b[38;5;241m.\u001b[39mReload()  \u001b[38;5;66;03m# loads events from file\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'ea' is not defined"]}],"source":["# Loading events from file...\n","with Timer():\n","    ea.Reload()  # loads events from file"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'ea' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tags \u001b[38;5;241m=\u001b[39m \u001b[43mea\u001b[49m\u001b[38;5;241m.\u001b[39mTags()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m''' Uncomment for logging \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mprint(' ')\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mprint('Log summary:')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    print('   ' + t + ': ' + tagSum)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n","\u001b[1;31mNameError\u001b[0m: name 'ea' is not defined"]}],"source":["tags = ea.Tags()\n","''' Uncomment for logging \n","print(' ')\n","print('Log summary:')\n","for t in tags:\n","    tagSum = []\n","    if isinstance(tags[t], collections.Sequence):\n","        tagSum = str(len(tags[t])) + ' summaries'\n","    else:\n","        tagSum = str(tags[t])\n","    print('   ' + t + ': ' + tagSum)\n","'''"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'outputFolder' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[43moutputFolder\u001b[49m):\n\u001b[0;32m      2\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(outputFolder)\n","\u001b[1;31mNameError\u001b[0m: name 'outputFolder' is not defined"]}],"source":["if not os.path.isdir(outputFolder):\n","    os.makedirs(outputFolder)"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["if 'audio' in summaries:\n","    print(' ')\n","    print('Exporting audio...')\n","    with Timer():\n","        print('   Audio is not yet supported!')"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["if 'compressedHistograms' in summaries:\n","    print(' ')\n","    print('Exporting compressedHistograms...')\n","    with Timer():\n","        print('   Compressed histograms are not yet supported!')"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["if 'histograms' in summaries:\n","    print(' ')\n","    print('Exporting histograms...')\n","    with Timer():\n","        print('   Histograms are not yet supported!')"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["if 'images' in summaries:\n","    print(' ')\n","    print('Exporting images...')\n","    imageDir = outputFolder + 'images'\n","    print('Image dir: ' + imageDir)\n","    with Timer():\n","        imageTags = tags['images']\n","        for imageTag in imageTags:\n","            images = ea.Images(imageTag)\n","            imageTagDir = imageDir + '/' + imageTag\n","            if not os.path.isdir(imageTagDir):\n","                os.makedirs(imageTagDir)\n","            for image in images:\n","                imageFilename = imageTagDir + '/' + str(image.step) + '.png'\n","                with open(imageFilename, 'wb') as f:\n","                    f.write(image.encoded_image_string)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'outputFolder' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscalars\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m summaries:\n\u001b[1;32m----> 2\u001b[0m     csvFileName \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43moutputFolder\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscalars.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Exporting scalars to csv-file...\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCSV-path: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m csvFileName)\n","\u001b[1;31mNameError\u001b[0m: name 'outputFolder' is not defined"]}],"source":["if 'scalars' in summaries:\n","    csvFileName = os.path.join(outputFolder, 'scalars.csv')\n","    # Exporting scalars to csv-file...\n","    print('CSV-path: ' + csvFileName)\n","    scalarTags = tags['scalars']\n","    with Timer():\n","        with open(csvFileName, 'w') as csvfile:\n","            logWriter = csv.writer(csvfile, delimiter=',')\n","\n","            # Write headers to columns\n","            headers = ['wall_time', 'step']\n","            for s in scalarTags:\n","                headers.append(s)\n","            logWriter.writerow(headers)\n","            vals = ea.Scalars(scalarTags[0])\n","            for i in range(len(vals)):\n","                v = vals[i]\n","                data = [v.wall_time, v.step]\n","                for s in scalarTags:\n","                    scalarTag = ea.Scalars(s)\n","                    S = scalarTag[i]\n","                    data.append(S.value)\n","                logWriter.writerow(data)"]},{"cell_type":"markdown","metadata":{},"source":["#knapsack_implementation"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["def knapSack(W, wt, val, n):\n","\t\"\"\" Maximize the value that a knapsack of capacity W can hold. You can either put the item or discard it, there is\n","\tno concept of putting some part of item in the knapsack.\n","\n","\t:param int W: Maximum capacity -in frames- of the knapsack.\n","\t:param list[int] wt: The weights (lengths -in frames-) of each video shot.\n","\t:param list[float] val: The values (importance scores) of each video shot.\n","\t:param int n: The number of the shots.\n","\t:return: A list containing the indices of the selected shots.\n","\t\"\"\"\n","\tK = [[0 for _ in range(W + 1)] for _ in range(n + 1)]\n","\n","\t# Build table K[][] in bottom up manner\n","\tfor i in range(n + 1):\n","\t\tfor w in range(W + 1):\n","\t\t\tif i == 0 or w == 0:\n","\t\t\t\tK[i][w] = 0\n","\t\t\telif wt[i - 1] <= w:\n","\t\t\t\tK[i][w] = max(val[i - 1] + K[i - 1][w - wt[i - 1]], K[i - 1][w])\n","\t\t\telse:\n","\t\t\t\tK[i][w] = K[i - 1][w]\n","\n","\tselected = []\n","\tw = W\n","\tfor i in range(n, 0, -1):\n","\t\tif K[i][w] != K[i - 1][w]:\n","\t\t\tselected.insert(0, i - 1)\n","\t\t\tw -= wt[i - 1]\n","\n","\treturn selected\n","\n","\n","if __name__ == \"__main__\":\n","\tpass\n","\t\"\"\" Driver program to test above function\n","\tval = [4, 4, 2, 2, 2, 4]\n","\twt =  [2, 2, 1, 1, 1, 2]\n","\tW = 7\n","\tn = len(val)\n","\tselected = knapSack(W, wt, val, n)\n","\tprint(selected) \n","\t\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["#configs"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","import argparse\n","import torch\n","from pathlib import Path\n","import pprint"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["save_dir = Path('C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/summaries/exp1')"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["def str2bool(v):\n","    \"\"\" Transcode string to boolean.\n","    :param str v: String to be transcoded.\n","    :return: The boolean transcoding of the string.\n","    \"\"\"\n","    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n","        return True\n","    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n","        return False\n","    else:\n","        raise argparse.ArgumentTypeError('Boolean value expected.')"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["class Config(object):\n","    def __init__(self, **kwargs):\n","        \"\"\" Configuration Class: set kwargs as class attributes with setattr. \"\"\"\n","        self.log_dir, self.score_dir, self.save_dir = None, None, None\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        for k, v in kwargs.items():\n","            setattr(self, k, v)\n","        self.set_dataset_dir(self.reg_factor, self.video_type)\n","    def set_dataset_dir(self, reg_factor=0.6, video_type='SumMe'):\n","        \"\"\" Function that sets as class attributes the necessary directories for logging important training information.\n","        :param float reg_factor: The utilized length regularization factor.\n","        :param str video_type: The Dataset being used, SumMe or TVSum.\n","        \"\"\"\n","        self.log_dir = save_dir.joinpath('reg' + str(reg_factor), video_type, 'logs/split' + str(self.split_index))\n","        self.score_dir = save_dir.joinpath('reg' + str(reg_factor), video_type, 'results/split' + str(self.split_index))\n","        self.save_dir = save_dir.joinpath('reg' + str(reg_factor), video_type, 'models/split' + str(self.split_index))\n","    def __repr__(self):\n","        \"\"\" Pretty-print configurations in alphabetical order. \"\"\"\n","        config_str = 'Configurations\\n'\n","        config_str += pprint.pformat(self.__dict__)\n","        return config_str"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["def get_config(parse=True, **optional_kwargs):\n","    \"\"\" Get configurations as attributes of class\n","        1. Parse configurations with argparse.\n","        2. Create Config class initialized with parsed kwargs.\n","        3. Return Config class.\n","    \"\"\"\n","    # If using argparse to parse command line arguments, uncomment the following:\n","    # parser = argparse.ArgumentParser()\n","\n","    # # Mode\n","    # parser.add_argument('--mode', type=str, default='train', help='Mode for the configuration [train | test]')\n","    # parser.add_argument('--verbose', type=str2bool, default='false', help='Print or not training messages')\n","    # parser.add_argument('--video_type', type=str, default='SumMe', help='Dataset to be used')\n","\n","    # # Model\n","    # parser.add_argument('--input_size', type=int, default=1024, help='Feature size expected in the input')\n","    # parser.add_argument('--block_size', type=int, default=60, help=\"Size of blocks used inside the attention matrix\")\n","    # parser.add_argument('--init_type', type=str, default=\"xavier\", help='Weight initialization method')\n","    # parser.add_argument('--init_gain', type=float, default=1.4142, help='Scaling factor for the initialization methods')\n","\n","    # # Train\n","    # parser.add_argument('--n_epochs', type=int, default=400, help='Number of training epochs')\n","    # parser.add_argument('--batch_size', type=int, default=20, help='Size of each batch in training')\n","    # parser.add_argument('--seed', type=int, default=12345, help='Chosen seed for generating random numbers')\n","    # parser.add_argument('--clip', type=float, default=5.0, help='Max norm of the gradients')\n","    # parser.add_argument('--lr', type=float, default=5e-4, help='Learning rate used for the modules')\n","    # parser.add_argument('--l2_req', type=float, default=1e-5, help='Weight regularization factor')\n","    # parser.add_argument('--reg_factor', type=float, default=0.6, help='Length regularization factor')\n","    # parser.add_argument('--split_index', type=int, default=0, help='Data split to be used [0-4]')\n","\n","    # if parse:\n","    #     kwargs = parser.parse_args()\n","    # else:\n","    #     kwargs = parser.parse_known_args()[0]\n","    # args, unknown = parser.parse_known_args()\n","\n","    # # Namespace => Dictionary\n","    # kwargs = vars(kwargs)\n","    \n","    # Example of hardcoded arguments\n","    kwargs = {\n","        \"mode\": \"train\",\n","        \"verbose\": 'false',\n","        \"video_type\": \"SumMe\",\n","        # Model\n","        \"input_size\": 1024,\n","        \"block_size\": 60,\n","        \"init_type\": \"xavier\",\n","        \"init_gain\": 1.4142,\n","        # Train\n","        \"n_epochs\": 1,\n","        \"batch_size\": 20,\n","        \"seed\": 12345,\n","        \"clip\": 5.0,\n","        \"lr\": 5e-4,\n","        \"l2_req\": 1e-5,\n","        \"reg_factor\": 0.6,\n","        \"dataset\": r\"C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\data\\SumMe\\eccv16_dataset_summe_google_pool5.h5\",\n","        \"split_index\": 0,\n","        \"beta\":0.01,\n","        \"weight_decay\":1e-05,\n","        \"num_episode\":5\n","\n","    }\n","    \n","    kwargs.update(optional_kwargs)\n","    \n","    # Ensure Config class is defined somewhere\n","    return Config(**kwargs)\n","\n","\n"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["if __name__ == '__main__':\n","    config = get_config()\n","    # import ipdb   \n","    # ipdb.set_trace()"]},{"cell_type":"markdown","metadata":{},"source":["#uutils"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["from __future__ import absolute_import\n","import os\n","import sys\n","import errno\n","import shutil\n","import json\n","import os.path as osp\n","import torch"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["def mkdir_if_missing(directory):\n","    if not osp.exists(directory):\n","        try:\n","            os.makedirs(directory)\n","        except OSError as e:\n","            if e.errno != errno.EEXIST:\n","                raise"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value.\n","       \n","       Code imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n","    \"\"\"\n","    def __init__(self):\n","        self.reset()\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["def save_checkpoint(state, fpath='checkpoint.pth.tar'):\n","    mkdir_if_missing(osp.dirname(fpath))\n","    torch.save(state, fpath)"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["class Logger(object):\n","    \"\"\"\n","    Write console output to external text file.\n","    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/utils/logging.py.\n","    \"\"\"\n","    def __init__(self, fpath=None):\n","        self.console = sys.stdout\n","        self.file = None\n","        if fpath is not None:\n","            mkdir_if_missing(os.path.dirname(fpath))\n","            self.file = open(fpath, 'w')\n","    def __del__(self):\n","        self.close()\n","    def __enter__(self):\n","        pass\n","    def __exit__(self, *args):\n","        self.close()\n","    def write(self, msg):\n","        self.console.write(msg)\n","        if self.file is not None:\n","            self.file.write(msg)\n","    def flush(self):\n","        self.console.flush()\n","        if self.file is not None:\n","            self.file.flush()\n","            os.fsync(self.file.fileno())\n","    def close(self):\n","        self.console.close()\n","        if self.file is not None:\n","            self.file.close()"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["def read_json(fpath):\n","    with open(fpath, 'r') as f:\n","        obj = json.load(f)\n","    return obj"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["def write_json(obj, fpath):\n","    mkdir_if_missing(osp.dirname(fpath))\n","    with open(fpath, 'w') as f:\n","        json.dump(obj, f, indent=4, separators=(',', ': '))"]},{"cell_type":"markdown","metadata":{},"source":["#create_split"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["from __future__ import print_function\n","import os\n","import os.path as osp\n","import argparse\n","import h5py\n","import math\n","import numpy as np"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["args = { \"dataset\":r\"C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\data\\SumMe\\eccv16_dataset_summe_google_pool5.h5\",\n","\"save_dir\": r\"C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\",\n","\"save_name\": 'splits',\n","\"num_splits\": 5,\n","\"train_percent\": 0.8}"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["def split_random(keys, num_videos, num_train):\n","    \"\"\"Random split\"\"\"\n","    train_keys, test_keys = [], []\n","    rnd_idxs = np.random.choice(range(num_videos), size=num_train, replace=False)\n","    for key_idx, key in enumerate(keys):\n","        if key_idx in rnd_idxs:\n","            train_keys.append(key)\n","        else:\n","            test_keys.append(key)\n","    assert len(set(train_keys) & set(test_keys)) == 0, \"Error: train_keys and test_keys overlap\"\n","    return train_keys, test_keys"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["def create():\n","    print(\"==========\\nArgs:{}\\n==========\".format(args))\n","    print(\"Goal: randomly split data for {} times, {:.1%} for training and the rest for testing\".format(args[\"num_splits\"], args[\"train_percent\"]))\n","    print(\"Loading dataset from {}\".format(args[\"dataset\"]))\n","    dataset = h5py.File(args[\"dataset\"], 'r')\n","    keys = dataset.keys()\n","    num_videos = len(keys)\n","    num_train = int(math.ceil(num_videos * args[\"train_percent\"]))\n","    num_test = num_videos - num_train\n","    print(\"Split breakdown: # total videos {}. # train videos {}. # test videos {}\".format(num_videos, num_train, num_test))\n","    splits = []\n","    for split_idx in range(args[\"num_splits\"]):\n","        train_keys, test_keys = split_random(keys, num_videos, num_train)\n","        splits.append({\n","            'train_keys': train_keys,\n","            'test_keys': test_keys,\n","            })\n","    saveto = osp.join(args[\"save_dir\"], args[\"save_name\"] + '.json')\n","    write_json(splits, saveto)\n","    print(\"Splits saved to {}\".format(saveto))\n","    dataset.close()"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["==========\n","Args:{'dataset': 'C:\\\\Users\\\\abhis\\\\OneDrive\\\\Desktop\\\\video summarization\\\\CA-SUM-main\\\\data\\\\SumMe\\\\eccv16_dataset_summe_google_pool5.h5', 'save_dir': 'C:\\\\Users\\\\abhis\\\\OneDrive\\\\Desktop\\\\video summarization\\\\CA-SUM-main\\\\summaries\\\\summaries', 'save_name': 'splits', 'num_splits': 5, 'train_percent': 0.8}\n","==========\n","Goal: randomly split data for 5 times, 80.0% for training and the rest for testing\n","Loading dataset from C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\data\\SumMe\\eccv16_dataset_summe_google_pool5.h5\n","Split breakdown: # total videos 25. # train videos 20. # test videos 5\n","Splits saved to C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\splits.json\n"]}],"source":["if __name__ == '__main__':\n","    create()"]},{"cell_type":"markdown","metadata":{},"source":["#data_loader"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import h5py\n","import numpy as np\n","import json"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["class VideoData(Dataset):\n","    def __init__(self, mode, video_type, split_index):\n","        \"\"\" Custom Dataset class wrapper for loading the frame features.\n","        :param str mode: The mode of the model, train or test.\n","        :param str video_type: The Dataset being used, SumMe or TVSum.\n","        :param int split_index: The index of the Dataset split being used.\n","        \"\"\"\n","        self.mode = mode\n","        self.name = video_type.lower()\n","        self.datasets = [r'C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/data/SumMe/eccv16_dataset_summe_google_pool5.h5',\n","                         r'C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/data/TVSum/eccv16_dataset_tvsum_google_pool5.h5']\n","        self.splits_filename = [r'C:/Users/abhis/OneDrive/Desktop/video summarization/CA-SUM-main/data/splits/' + self.name + r'_splits.json']\n","        self.split_index = split_index\n","        if 'summe' in self.splits_filename[0]:\n","            self.filename = self.datasets[0]\n","        elif 'tvsum' in self.splits_filename[0]:\n","            self.filename = self.datasets[1]\n","        hdf = h5py.File(self.filename, 'r')\n","        self.list_frame_features = []\n","        with open(self.splits_filename[0]) as f:\n","            data = json.loads(f.read())\n","            for i, split in enumerate(data):\n","                if i == self.split_index:\n","                    self.split = split\n","                    break\n","        for video_name in self.split[self.mode + '_keys']:\n","            frame_features = torch.Tensor(np.array(hdf[video_name + '/features']))\n","            self.list_frame_features.append(frame_features)\n","        hdf.close()\n","    def __len__(self):\n","        \"\"\" Function to be called for the `len` operator of `VideoData` Dataset. \"\"\"\n","        self.len = len(self.split[self.mode+'_keys'])\n","        return self.len\n","    \n","    def __getitem__(self, index):\n","        \"\"\" Function to be called for the index operator of `VideoData` Dataset.\n","        train mode returns: frame_features\n","        test  mode returns: frame_features and video name\n","        :param int index: The above-mentioned id of the data.\n","        \"\"\"\n","        frame_features = self.list_frame_features[index]\n","        if self.mode == 'test':\n","            video_name = self.split[self.mode + '_keys'][index]\n","            return frame_features, video_name\n","        else:\n","            return frame_features"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["def get_loader(mode, video_type, split_index):\n","    \"\"\" Loads the `data.Dataset` of the `split_index` split for the `video_type` Dataset.\n","    Wrapped by a Dataloader, shuffled and `batch_size` = 1 in train `mode`.\n","    :param str mode: The mode of the model, train or test.\n","    :param str video_type: The Dataset being used, SumMe or TVSum.\n","    :param int split_index: The index of the Dataset split being used.\n","    :return: The Dataset used in each mode.\n","    \"\"\"\n","    if mode.lower() == 'train':\n","        vd = VideoData(mode, video_type, split_index)\n","        return DataLoader(vd, batch_size=1, shuffle=True)\n","    else:\n","        return VideoData(mode, video_type, split_index)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["if __name__ == '__main__':\n","    pass"]},{"cell_type":"markdown","metadata":{},"source":["#rewards for RL"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["import torch\n","import sys\n","\n","def compute_reward(seq, actions, ignore_far_sim=True, temp_dist_thre=20, use_gpu=False):\n","    \"\"\"\n","    Compute diversity reward and representativeness reward\n","\n","    Args:\n","        seq: sequence of features, shape (1, seq_len, dim)\n","        actions: binary action sequence, shape (1, seq_len, 1)\n","        ignore_far_sim (bool): whether to ignore temporally distant similarity (default: True)\n","        temp_dist_thre (int): threshold for ignoring temporally distant similarity (default: 20)\n","        use_gpu (bool): whether to use GPU\n","    \"\"\"\n","    _seq = seq.detach()\n","    _actions = actions.detach()\n","    pick_idxs = _actions.squeeze().nonzero().squeeze()\n","    num_picks = len(pick_idxs) if pick_idxs.ndimension() > 0 else 1\n","    \n","    if num_picks == 0:\n","        # give zero reward is no frames are selected\n","        reward = torch.tensor(0.)\n","        if use_gpu: reward = reward.cuda()\n","        return reward   \n","\n","    _seq = _seq.squeeze()\n","    n = _seq.size(0)\n","\n","    # compute diversity reward\n","    if num_picks == 1:\n","        reward_div = torch.tensor(0.)\n","        if use_gpu: reward_div = reward_div.cuda()\n","    else:\n","        normed_seq = _seq / _seq.norm(p=2, dim=1, keepdim=True)\n","        dissim_mat = 1. - torch.matmul(normed_seq, normed_seq.t()) # dissimilarity matrix [Eq.4]\n","        dissim_submat = dissim_mat[pick_idxs,:][:,pick_idxs]\n","        if ignore_far_sim:\n","            # ignore temporally distant similarity\n","            pick_mat = pick_idxs.expand(num_picks, num_picks)\n","            temp_dist_mat = torch.abs(pick_mat - pick_mat.t())\n","            dissim_submat[temp_dist_mat > temp_dist_thre] = 1.\n","        reward_div = dissim_submat.sum() / (num_picks * (num_picks - 1.)) # diversity reward [Eq.3]\n","\n","    # compute representativeness reward\n","    dist_mat = torch.pow(_seq, 2).sum(dim=1, keepdim=True).expand(n, n)\n","    dist_mat = dist_mat + dist_mat.t()\n","    dist_mat.addmm_(1, -2, _seq, _seq.t())\n","    dist_mat = dist_mat[:,pick_idxs]\n","    dist_mat = dist_mat.min(1, keepdim=True)[0]\n","    #reward_rep = torch.exp(torch.FloatTensor([-dist_mat.mean()]))[0] # representativeness reward [Eq.5]\n","    reward_rep = torch.exp(-dist_mat.mean())\n","\n","    # combine the two rewards\n","    reward = (reward_div + reward_rep) * 0.5\n","\n","    return reward\n"]},{"cell_type":"markdown","metadata":{},"source":["#solver"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import os\n","import random\n","import json\n","import h5py\n","from tqdm import tqdm, trange\n"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["class Solver(object):\n","    def __init__(self, config=None, train_loader=None, test_loader=None):\n","        \"\"\" Class that Builds, Trains and Evaluates CA-SUM model. \"\"\"\n","        # Initialize variables to None, to be safe\n","        self.model, self.optimizer, self.writer = None, None, None\n","        self.config = config\n","        self.train_loader = train_loader\n","        self.test_loader = test_loader\n","\n","        # Set the seed for generating reproducible random numbers\n","        if self.config.seed is not None:\n","            torch.manual_seed(self.config.seed)\n","            torch.cuda.manual_seed_all(self.config.seed)\n","            np.random.seed(self.config.seed)\n","            random.seed(self.config.seed)\n","    def build(self):\n","        \"\"\" Function for constructing the CA-SUM model, its key modules and parameters. \"\"\"\n","        # Model creation    \n","        self.model = CA_SUM(input_size=self.config.input_size,\n","                            output_size=self.config.input_size,\n","                            block_size=self.config.block_size).to(self.config.device)\n","        if self.config.init_type is not None:\n","            self.init_weights(net=self.model, init_type=self.config.init_type, init_gain=self.config.init_gain)\n","        if self.config.mode == 'train':\n","            self.optimizer = optim.Adam(self.model.parameters(), lr=self.config.lr, weight_decay=self.config.l2_req)\n","            self.writer = TensorboardWriter(str(self.config.log_dir))\n","    @staticmethod\n","    def init_weights(net, init_type=\"xavier\", init_gain=1.4142):\n","        \"\"\" Initialize 'net' network weights, based on the chosen 'init_type' and 'init_gain'.\n","        :param nn.Module net: Network to be initialized.\n","        :param str init_type: Name of initialization method: normal | xavier | kaiming | orthogonal.\n","        :param float init_gain: Scaling factor for normal.\n","        \"\"\"\n","        for name, param in net.named_parameters():\n","            if 'weight' in name and \"norm\" not in name:\n","                if init_type == \"normal\":\n","                    nn.init.normal_(param, mean=0.0, std=init_gain)\n","                elif init_type == \"xavier\":\n","                    nn.init.xavier_uniform_(param, gain=np.sqrt(2.0))  # ReLU activation function\n","                elif init_type == \"kaiming\":\n","                    nn.init.kaiming_uniform_(param, mode=\"fan_in\", nonlinearity=\"relu\")\n","                elif init_type == \"orthogonal\":\n","                    nn.init.orthogonal_(param, gain=np.sqrt(2.0))      # ReLU activation function\n","                else:\n","                    raise NotImplementedError(f\"initialization method {init_type} is not implemented.\")\n","            elif 'bias' in name:\n","                nn.init.constant_(param, 0.1)\n","    def length_regularization_loss(self, scores):\n","        \"\"\" Compute the summary-length regularization loss based on eq. (1).\n","        :param torch.Tensor scores: Frame-level importance scores, produced by our CA-SUM model.\n","        :return: A (torch.Tensor) value indicating the summary-length regularization loss.\n","        \"\"\"\n","        return torch.abs(torch.mean(scores) - self.config.reg_factor)\n","    def train(self):\n","        \"\"\" Main function to train the CA-SUM model. \"\"\"\n","        if self.config.verbose:\n","            tqdm.write('Time to train the model...')\n","        for epoch_i in trange(self.config.n_epochs, desc='Epoch', ncols=80):\n","            self.model.train()\n","            loss_history = []\n","            num_batches = int(len(self.train_loader) / self.config.batch_size)  # full-batch or mini batch\n","            iterator = iter(self.train_loader)\n","            for _ in trange(num_batches, desc='Batch', ncols=80, leave=False):\n","                self.optimizer.zero_grad()\n","                for _ in trange(self.config.batch_size, desc='Video', ncols=80, leave=False):\n","                    frame_features = next(iterator)\n","                    frame_features = frame_features.squeeze(0).to(self.config.device)\n","                    output, _ = self.model(frame_features)\n","                    loss = self.length_regularization_loss(output)\n","                    loss_history.append(loss.data)\n","                    loss.backward()\n","                # Update model parameters every 'batch_size' iterations\n","                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.clip)\n","                self.optimizer.step()\n","\n","            # Mean loss of each training step\n","            loss = torch.stack(loss_history).mean()\n","            if self.config.verbose:\n","                tqdm.write(f'[{epoch_i}] loss: {loss.item()}')\n","\n","            # Plot\n","            if self.config.verbose:\n","                tqdm.write('Plotting...')\n","            self.writer.update_loss(loss, epoch_i, 'loss_epoch')\n","\n","            # Uncomment to save parameters at checkpoint\n","            if not os.path.exists(self.config.save_dir):\n","                os.makedirs(self.config.save_dir)\n","            ckpt_path = str(self.config.save_dir) + f'/epoch-{epoch_i}.pkl'\n","            tqdm.write(f'Save parameters at {ckpt_path}')\n","            torch.save(self.model.state_dict(), ckpt_path)\n","            self.evaluate(epoch_i)\n","    def evaluate(self, epoch_i, save_weights=False):\n","        \"\"\" Saves the frame's importance scores for the test videos in json format.\n","        :param int epoch_i: The current training epoch.\n","        :param bool save_weights: Optionally, the user can choose to save the attention weights in a (large) h5 file.\n","        \"\"\"\n","        self.model.eval()\n","        weights_save_path = self.config.score_dir.joinpath(\"weights.h5\")\n","        out_scores_dict = {}\n","        for frame_features, video_name in tqdm(self.test_loader, desc='Evaluate', ncols=80, leave=False):\n","            # [seq_len, input_size]\n","            frame_features = frame_features.view(-1, self.config.input_size).to(self.config.device)\n","            with torch.no_grad():\n","                scores, attn_weights = self.model(frame_features)  # [1, seq_len]\n","                scores = scores.squeeze(0).cpu().numpy().tolist()\n","                attn_weights = attn_weights.cpu().numpy()\n","                out_scores_dict[video_name] = scores\n","            if not os.path.exists(self.config.score_dir):\n","                os.makedirs(self.config.score_dir)\n","            scores_save_path = self.config.score_dir.joinpath(f\"{self.config.video_type}_{epoch_i}.json\")\n","            with open(scores_save_path, 'w') as f:\n","                if self.config.verbose:\n","                    tqdm.write(f'Saving score at {str(scores_save_path)}.')\n","                json.dump(out_scores_dict, f)\n","            scores_save_path.chmod(0o777)\n","            if save_weights and (epoch_i+1 == self.config.n_epochs or epoch_i+1 == 0):\n","                with h5py.File(weights_save_path, 'a') as weights:\n","                    weights.create_dataset(f\"{video_name}/epoch_{epoch_i}\", data=attn_weights)"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["if __name__ == '__main__':\n","    pass"]},{"cell_type":"markdown","metadata":{},"source":["#utils"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","from tensorboardX import SummaryWriter"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["class TensorboardWriter(SummaryWriter):\n","    def __init__(self, logdir):\n","        \"\"\" Extended SummaryWriter Class from tensorboard-pytorch (tensorboardX)\n","        https://github.com/lanpa/tensorboard-pytorch/blob/master/tensorboardX/writer.py\n","        Internally calls self.file_writer\n","        :param str logdir: Save directory location.\n","        \"\"\"\n","        super(TensorboardWriter, self).__init__(logdir)\n","        self.logdir = self.file_writer.get_logdir()\n","    def update_parameters(self, module, step_i):\n","        \"\"\" Add module's parameters' histogram to summary.\n","        :param torch.nn.Module module: Module from which the parameters will be taken.\n","        :param int step_i: Step value to record.\n","        \"\"\"\n","        for name, param in module.named_parameters():\n","            self.add_histogram(name, param.clone().cpu().data.numpy(), step_i)\n","    def update_loss(self, loss, step_i, name='loss'):\n","        \"\"\" Add scalar data to summary.\n","        :param float loss: Value to save.\n","        :param int step_i: Step value to record.\n","        :param str name: Data identifier.\n","        \"\"\"\n","        self.add_scalar(name, loss, step_i)\n","    def update_histogram(self, values, step_i, name='hist'):\n","        \"\"\" Add histogram to summary.\n","        :param torch.Tensor | numpy.ndarray values: Values to build histogram.\n","        :param int step_i: Step value to record.\n","        :param str name: Data identifier.\n","        \"\"\"\n","        self.add_histogram(name, values, step_i)"]},{"cell_type":"markdown","metadata":{},"source":["#main"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.backends.cudnn as cudnn\n","from torch.optim import lr_scheduler\n","from torch.distributions import Bernoulli\n","import numpy as np"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["if __name__ == '__main__':\n","    \"\"\" Main function that sets the data loaders; trains and evaluates the solver.\"\"\"\n","    config = get_config(mode='train')"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Current split: 0]: block_size=60 and σ=0.6 for SumMe dataset.\n"]},{"name":"stderr","output_type":"stream","text":["Evaluate:   0%|                                           | 0/5 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_-1.json.\n"]},{"name":"stderr","output_type":"stream","text":["Evaluate:  80%|████████████████████████████       | 4/5 [00:00<00:00, 11.65it/s]"]},{"name":"stdout","output_type":"stream","text":["Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_-1.json.\n","Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_-1.json.\n","Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_-1.json.\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_-1.json.\n","Time to train the model...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","Epoch:   0%|                                              | 0/1 [00:04<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["[0] loss: 0.11152105033397675\n","Plotting...\n","Save parameters at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\models\\split0/epoch-0.pkl\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch:   0%|                                              | 0/1 [00:04<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_0.json.\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch:   0%|                                              | 0/1 [00:04<?, ?it/s]\n","Epoch:   0%|                                              | 0/1 [00:04<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_0.json.\n","Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_0.json.\n"]},{"name":"stderr","output_type":"stream","text":["\n","Epoch:   0%|                                              | 0/1 [00:04<?, ?it/s]\n","Epoch:   0%|                                              | 0/1 [00:05<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_0.json.\n","Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_0.json.\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 100%|██████████████████████████████████████| 1/1 [00:05<00:00,  5.12s/it]\n"]}],"source":["if __name__ == '__main__':\n","    \"\"\" Main function that sets the data loaders; trains and evaluates the solver.\"\"\"\n","    config = get_config(mode='train')\n","    test_config = get_config(mode='test')\n","    print(f\"[Current split: {config.split_index}]: block_size={config.block_size} and \"\n","          f\"\\u03C3={config.reg_factor} for {config.video_type} dataset.\")\n","    train_loader = get_loader(config.mode, config.video_type, config.split_index)\n","    test_loader = get_loader(test_config.mode, test_config.video_type, test_config.split_index)\n","    solver = Solver(config, train_loader, test_loader)\n","    solver.build()\n","    solver.evaluate(-1)\t # evaluates the summaries using the initial random weights of the network\n","    solver.train()\n","  \n","    dataset = h5py.File(config.dataset, 'r')\n","    num_videos = len(dataset.keys())\n","    splits = read_json(r\"C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\data\\splits\\summe_splits.json\")\n","    \n","    assert config.split_index < len(splits), \"split_id (got {}) exceeds {}\".format(config.split_index, len(splits))\n","    split = splits[config.split_index]\n","    train_keys = split['train_keys']\n","    test_keys = split['test_keys']\n","    start_epoch=0\n","\n","# tensorboard --logdir '.../CA-SUM/Summaries/' --host localhost\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# RL for Summarization "]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\abhis\\AppData\\Local\\Temp\\ipykernel_18804\\1799163324.py:47: UserWarning: This overload of addmm_ is deprecated:\n","\taddmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n","Consider using one of the following signatures instead:\n","\taddmm_(Tensor mat1, Tensor mat2, *, Number beta, Number alpha) (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:1630.)\n","  dist_mat.addmm_(1, -2, _seq, _seq.t())\n"]},{"name":"stdout","output_type":"stream","text":["epoch 1/1\t reward 0.9309951132535934\t\n"]},{"name":"stderr","output_type":"stream","text":["Evaluate:  20%|███████                            | 1/5 [00:00<00:00,  8.26it/s]"]},{"name":"stdout","output_type":"stream","text":["Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_-1.json.\n"]},{"name":"stderr","output_type":"stream","text":["Evaluate:  80%|████████████████████████████       | 4/5 [00:00<00:00,  8.54it/s]"]},{"name":"stdout","output_type":"stream","text":["Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_-1.json.\n","Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_-1.json.\n","Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_-1.json.\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_-1.json.\n","Time to train the model...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch:   0%|                                              | 0/1 [00:00<?, ?it/s]\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","\u001b[A\n","Epoch:   0%|                                              | 0/1 [00:04<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["[0] loss: 0.3994632959365845\n","Plotting...\n","Save parameters at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\models\\split0/epoch-0.pkl\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \n","Epoch:   0%|                                              | 0/1 [00:04<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_0.json.\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \n","                                                                                \n","                                                                                \n","Epoch:   0%|                                              | 0/1 [00:05<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_0.json.\n","Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_0.json.\n","Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_0.json.\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \n","Epoch: 100%|██████████████████████████████████████| 1/1 [00:05<00:00,  5.44s/it]"]},{"name":"stdout","output_type":"stream","text":["Saving score at C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\summaries\\summaries\\reg0.6\\SumMe\\results\\split0\\SumMe_0.json.\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["\n","baselines = {key: 0. for key in train_keys} # baseline rewards for videos\n","reward_writers = {key: [] for key in train_keys} # record reward changes for each video\n","for epoch in range(start_epoch, config.n_epochs):\n","        idxs = np.arange(len(train_keys))\n","        np.random.shuffle(idxs) # shuffle indices\n","        for idx in idxs:\n","            key = train_keys[idx]\n","            seq = dataset[key]['features'][...] # sequence of features, (seq_len, dim)\n","            seq = torch.from_numpy(seq) # input shape (seq_len, dim)  \n","            #\n","            trained_model = solver.model\n","            # input=torch.randn(100,1024)\n","            y,attn_weights = trained_model(seq) # output shape (1, seq_len, 1)\n","            probs = y.squeeze()\n","            #*\n","            cost = config.beta * (probs.mean() - 0.5)**2 # minimize summary length penalty term [Eq.11]\n","            m = Bernoulli(probs)\n","            epis_rewards = []\n","            for _ in range(config.num_episode):\n","                actions = m.sample()\n","                log_probs = m.log_prob(actions)\n","                #* \n","                use_gpu=False\n","                reward = compute_reward(seq, actions, use_gpu=use_gpu)\n","                expected_reward = log_probs.mean() * (reward - baselines[key])\n","                cost -= expected_reward # minimize negative expected reward\n","                epis_rewards.append(reward.item())\n","            optimizer = torch.optim.Adam(trained_model.parameters(), lr=config.lr, weight_decay=config.weight_decay)    \n","            optimizer.zero_grad()\n","            cost.backward()\n","            torch.nn.utils.clip_grad_norm_(trained_model.parameters(), 5.0)\n","            optimizer.step()\n","            baselines[key] = 0.9 * baselines[key] + 0.1 * np.mean(epis_rewards) # update baseline reward via moving average\n","            reward_writers[key].append(np.mean(epis_rewards))\n","        epoch_reward = np.mean([reward_writers[key][epoch] for key in train_keys])\n","        print(\"epoch {}/{}\\t reward {}\\t\".format(epoch+1, config.n_epochs, epoch_reward))\n","write_json(reward_writers, osp.join(config.save_dir, 'rewards.json'))\n","solver.evaluate(-1)\t # evaluates the summaries using the initial random weights of the network\n","solver.train()\n","  \n","dataset = h5py.File(config.dataset, 'r')\n","num_videos = len(dataset.keys())\n","splits = read_json(r\"C:\\Users\\abhis\\OneDrive\\Desktop\\video summarization\\CA-SUM-main\\data\\splits\\summe_splits.json\")\n","    \n","assert config.split_index < len(splits), \"split_id (got {}) exceeds {}\".format(config.split_index, len(splits))\n","split = splits[config.split_index]\n","train_keys = split['train_keys']\n","test_keys = split['test_keys']\n","start_epoch=0\n","\n","             \n","\n","\n","#changes \n","# seq = torch.from_numpy(seq).unsqueeze(0) # input shape (1,seq_len, dim) TO  seq = torch.from_numpy(seq) # input shape (seq_len, dim)  \n","# to fit in the input shape of CASUM model "]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["from __future__ import print_function\n","import os\n","import os.path as osp\n","import argparse\n","import sys\n","import h5py\n","import time\n","import datetime\n","import numpy as np\n","from tabulate import tabulate"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":2}
